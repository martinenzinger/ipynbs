{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-w_FnRdKq3d"
   },
   "source": [
    "<img width=\"260\" src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRcO6ATtznWETiIO6GoFD2SpmQfgILeMbxsTJe4P3sJ2xZgQReu\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQ6JUuCm4Ihc"
   },
   "outputs": [],
   "source": [
    "!pip install pillow\n",
    "!pip install tqdm retrowrapper gym-retro\n",
    "# copy your roms to data/roms/\n",
    "\n",
    "import retro\n",
    "!python -m retro.import data/roms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5Outje8AKBF"
   },
   "source": [
    "# Start the Game\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "colab_type": "code",
    "id": "2c7EUvr_Y4-i",
    "outputId": "7b843c24-bd21-48cc-ca2a-7fd86b8bfa6b"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import io\n",
    "import retro\n",
    "import time\n",
    "import base64\n",
    "from ipykernel.comm import Comm\n",
    "\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(a)\n",
    "    f = io.BytesIO()\n",
    "    ima = PIL.Image.fromarray(a).save(f, fmt)\n",
    "    return base64.b64encode(f.getvalue()).decode('ascii')\n",
    "\n",
    "\n",
    "nrplayers = 1\n",
    "verbosity = 0\n",
    "\n",
    "env = retro.make(game=selected_game, record=False, players=nrplayers)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "imagedata = showarray(env.render(mode='rgb_array'))\n",
    "\n",
    "jsc = \"Jupyter.notebook.kernel.comm_manager.register_target('gymscreen', function(comm, msg) { comm.on_msg(function(msg) { document.getElementById('gymscr').src = 'data:image/png;base64,'+msg.content.data.img_data; }); });\"\n",
    "\n",
    "imagehandle = display.display(display.HTML(\"<div><img id='gymscr' style='width:700px;height:520px;background-color:#000;' src='data:image/png;base64,\"+imagedata+\"'></div><script>\"+jsc+\"</script>\"), display_id='gymscreen_container')\n",
    "\n",
    "my_comm = Comm(target_name='gymscreen', data={'img_data': imagedata})\n",
    "\n",
    "\n",
    "\n",
    "print(env.action_space)\n",
    "print((env.render(mode='rgb_array')).shape)\n",
    "\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ob = env.reset()\n",
    "        t = 0\n",
    "        totrew = [0] * nrplayers\n",
    "        while True:\n",
    "            ac = env.action_space.sample()\n",
    "            ob, rew, done, info = env.step(ac)\n",
    "            t += 1\n",
    "            if t % 3 == 0:\n",
    "                my_comm.send({'img_data': imagedata })\n",
    "            else:\n",
    "                imagedata = showarray(env.render(mode='rgb_array'))\n",
    "            if t % 10 == 0:\n",
    "                if verbosity > 1:\n",
    "                    infostr = ''\n",
    "                    if info:\n",
    "                        infostr = ', info: ' + ', '.join(['%s=%i' % (k, v) for k, v in info.items()])\n",
    "                    print(('t=%i' % t) + infostr)\n",
    "            if nrplayers == 1:\n",
    "                rew = [rew]\n",
    "            for i, r in enumerate(rew):\n",
    "                totrew[i] += r\n",
    "                if verbosity > 0:\n",
    "                    if r > 0:\n",
    "                        print('t=%i p=%i got reward: %g, current reward: %g' % (t, i, r, totrew[i]))\n",
    "                    if r < 0:\n",
    "                        print('t=%i p=%i got penalty: %g, current reward: %g' % (t, i, r, totrew[i]))\n",
    "            if done:\n",
    "                endmsg = ''\n",
    "                try:\n",
    "                    if verbosity >= 0:\n",
    "                        if args.players > 1:\n",
    "                            endmsg = \"done! total reward: time=%i, reward=%r\" % (t, totrew)\n",
    "                        else:\n",
    "                            endmsg = \"done! total reward: time=%i, reward=%d\" % (t, totrew[0])\n",
    "                        input(\"press enter to continue\")\n",
    "                        env.close()\n",
    "                        print()\n",
    "                    else:\n",
    "                        input(\"\")\n",
    "                except EOFError:\n",
    "                    env.close\n",
    "                    exit(0)\n",
    "                break\n",
    "except KeyboardInterrupt:\n",
    "    env.close()\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUFcsqhG1X5O"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=560):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(state_size, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Dropout(0.3))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Dropout(0.3))\n",
    "        self.fc1 = nn.Linear(3*5*256, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, action_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        #state = state.unsqueeze(1)\n",
    "        #print(state.shape)\n",
    "        x = self.layer1(state)\n",
    "        x = self.layer2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(400)  # replay buffer size\n",
    "BATCH_SIZE = 40         # minibatch size\n",
    "GAMMA = 0.999            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 0.005              # learning rate \n",
    "UPDATE_EVERY = 20       # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.positiveqs_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.positiveqs_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.negativeqs_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.negativeqs_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer_pos = optim.SGD(self.positiveqs_local.parameters(), lr=LR)\n",
    "        self.optimizer_neg = optim.Adam(self.negativeqs_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"        \n",
    "        # only the green color channel\n",
    "        state = torch.from_numpy(np.stack([np.sum([np.multiply(0.2989, state[:,:,0]), np.multiply(0.5870 , state[:,:,1]), np.multiply(0.1140, state[:,:,2])], axis=0), state[:,:,0], state[:,:,1], state[:,:,2]])).float().unsqueeze(0).to(device)\n",
    "        self.positiveqs_local.eval()\n",
    "        self.negativeqs_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values_raw = self.positiveqs_local(state)\n",
    "            action_values_adjust = self.negativeqs_local(state)\n",
    "            action_values = np.maximum(action_values_raw - action_values_adjust, np.zeros(action_values_raw.shape))\n",
    "        self.positiveqs_local.train()\n",
    "        self.negativeqs_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            action_choice = action_values.cpu().data.numpy()\n",
    "            return np.random.choice(np.flatnonzero(action_choice == action_choice.max()))\n",
    "            #return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states_pos, actions_pos, rewards_pos, next_states_pos, dones_pos = experiences[0]\n",
    "        \n",
    "        if(len(next_states_pos) > 0):\n",
    "            # Get predicted actions from max Q values in local model\n",
    "            next_states_local_pos = self.positiveqs_local(next_states_pos).detach()\n",
    "            pred_actions_pos = next_states_local_pos.argmax(1)[:,None] == torch.from_numpy(np.arange(next_states_local_pos.shape[1]))\n",
    "            # Get predicted Q values (for predicted actions) from target model\n",
    "            Q_targets_next_pos = self.positiveqs_target(next_states_pos).detach()[pred_actions_pos].unsqueeze(1)\n",
    "            # Compute Q targets for current states \n",
    "            Q_targets_pos = rewards_pos + (gamma * Q_targets_next_pos * (1 - dones_pos))\n",
    "            # Get expected Q values from local model\n",
    "            Q_expected_pos = self.positiveqs_local(states_pos).gather(1, actions_pos)\n",
    "\n",
    "            # Compute loss\n",
    "            loss_pos = F.mse_loss(Q_expected_pos, Q_targets_pos)\n",
    "            # Minimize the loss\n",
    "            self.optimizer_pos.zero_grad()\n",
    "            loss_pos.backward()\n",
    "            self.optimizer_pos.step()\n",
    "\n",
    "\n",
    "        states_neg, actions_neg, rewards_neg, next_states_neg, dones_neg = experiences[1]\n",
    "\n",
    "        if(len(next_states_neg) > 0):\n",
    "            # Get predicted actions from max Q values in local model\n",
    "            next_states_local_neg = self.negativeqs_local(next_states_neg).detach()\n",
    "            pred_actions_neg = next_states_local_neg.argmax(1)[:,None] == torch.from_numpy(np.arange(next_states_local_neg.shape[1]))\n",
    "            # Get predicted Q values (for predicted actions) from target model\n",
    "            Q_targets_next_neg = self.negativeqs_target(next_states_neg).detach()[pred_actions_neg].unsqueeze(1)\n",
    "            # Compute Q targets for current states \n",
    "            Q_targets_neg = rewards_neg + (gamma * Q_targets_next_neg * (1 - dones_neg))\n",
    "            # Get expected Q values from local model\n",
    "            Q_expected_neg = self.negativeqs_local(states_neg).gather(1, actions_neg)\n",
    "\n",
    "            # Compute loss\n",
    "            loss_neg = F.mse_loss(Q_expected_neg, Q_targets_neg)\n",
    "            # Minimize the loss\n",
    "            self.optimizer_neg.zero_grad()\n",
    "            loss_neg.backward()\n",
    "            self.optimizer_neg.step()\n",
    "\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.positiveqs_local, self.positiveqs_target, self.negativeqs_local, self.negativeqs_target, TAU)                   \n",
    "\n",
    "    def soft_update(self, local_model_pos, target_model_pos, local_model_neg, target_model_neg, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param_pos, local_param_pos in zip(target_model_pos.parameters(), local_model_pos.parameters()):\n",
    "            target_param_pos.data.copy_(tau*local_param_pos.data + (1.0-tau)*target_param_pos.data)\n",
    "        for target_param_neg, local_param_neg in zip(target_model_neg.parameters(), local_model_neg.parameters()):\n",
    "            target_param_neg.data.copy_(tau*local_param_neg.data + (1.0-tau)*target_param_neg.data)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"        \n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.stack([np.stack([np.sum([np.multiply(0.2989, e.state[:,:,0]), np.multiply(0.5870 , e.state[:,:,1]), np.multiply(0.1140, e.state[:,:,2])], axis=0), e.state[:,:,0], e.state[:,:,1], e.state[:,:,2]]) for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([np.stack([np.sum([np.multiply(0.2989, e.state[:,:,0]), np.multiply(0.5870 , e.state[:,:,1]), np.multiply(0.1140 , e.state[:,:,2])], axis=0), e.state[:,:,0], e.state[:,:,1], e.state[:,:,2]]) for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "    \n",
    "        pos_idx = torch.from_numpy(np.array([x[0] for x in rewards > 0]))\n",
    "        neg_idx = torch.from_numpy(np.array([x[0] for x in rewards < -10]))\n",
    "        \n",
    "        return [(states[pos_idx], actions[pos_idx], rewards[pos_idx], next_states[pos_idx], dones[pos_idx]), \n",
    "                (states[neg_idx], actions[neg_idx], np.abs(rewards[neg_idx]), next_states[neg_idx], dones[neg_idx])]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=4, action_size=6, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import io\n",
    "import retro\n",
    "import time\n",
    "import base64\n",
    "from ipykernel.comm import Comm\n",
    "\n",
    "\n",
    "selected_game = 'SonicTheHedgehog-Genesis' #@param [\"SonicTheHedgehog-Genesis\", \"SonicTheHedgehog2-Genesis\", \"SonicAndKnuckles3-Genesis\"] {allow-input: true}\n",
    "\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(a)\n",
    "    f = io.BytesIO()\n",
    "    ima = PIL.Image.fromarray(a).save(f, fmt)\n",
    "    return base64.b64encode(f.getvalue()).decode('ascii')\n",
    "\n",
    "\n",
    "nrplayers = 1\n",
    "verbosity = 0\n",
    "\n",
    "env = retro.make(game=selected_game, record=False, players=nrplayers)\n",
    "env.reset()\n",
    "\n",
    "\n",
    "imagedata = showarray(env.render(mode='rgb_array'))\n",
    "jsc = \"Jupyter.notebook.kernel.comm_manager.register_target('gymscreen', function(comm, msg) { comm.on_msg(function(msg) { document.getElementById('gymscr').src = 'data:image/png;base64,'+msg.content.data.img_data; }); });\"\n",
    "imagehandle = display.display(display.HTML(\"<div><img id='gymscr' style='width:700px;height:520px;background-color:#000;' src='data:image/png;base64,\"+imagedata+\"'></div><script>\"+jsc+\"</script>\"), display_id='gymscreen_container')\n",
    "my_comm = Comm(target_name='gymscreen', data={'img_data': imagedata})\n",
    "\n",
    "\n",
    "#button_labels = [\"B\", \"A\", \"MODE\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"C\", \"Y\", \"X\", \"Z\"]\n",
    "button_combos = [['RIGHT'], ['LEFT'], ['RIGHT', 'UP'], ['DOWN', 'B'], ['B'], ['8 x RIGHT']]\n",
    "action_arrays = [[0,0,0,0,0,0,0,1,0,0,0,0], [0,0,0,0,0,0,1,0,0,0,0,0], [0,0,0,0,1,0,0,1,0,0,0,0], \n",
    "                 [1,0,0,0,0,1,0,0,0,0,0,0], [1,0,0,0,0,0,0,0,0,0,0,0]]\n",
    "\n",
    "\n",
    "def dqn(n_episodes=200, max_t=2500, eps_start=1.0, eps_end=0.1, eps_decay=0.05):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): factor for decreasing epsilon\n",
    "    \"\"\"\n",
    "    global imagedata\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        incentive = 0\n",
    "        rings = 0\n",
    "        screen_x_max = 0\n",
    "        state_prev = deque(maxlen=5)\n",
    "        action_prev = deque(maxlen=5)\n",
    "        foresight = deque(maxlen=5)\n",
    "        y_prev = 1000\n",
    "        lives = 3\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            incentive = 0\n",
    "            next_state_w = []\n",
    "            reward = 0\n",
    "            j = 2\n",
    "            if(action == 5):\n",
    "                j = 16\n",
    "                action = 0\n",
    "            for i in range(j):\n",
    "                next_st, rew, done, info = env.step(action_arrays[action])\n",
    "                next_state_w.append(next_st)\n",
    "                reward += rew\n",
    "            \n",
    "            rings_new = info['rings']\n",
    "            screen_x_new = info['screen_x']\n",
    "            lives_new = info['lives']\n",
    "            y_new = info['y']\n",
    "\n",
    "            if lives - lives_new == 1:\n",
    "                incentive -= 500\n",
    "            else:   \n",
    "                if rings_new != rings:\n",
    "                    if rings_new == 0:\n",
    "                        incentive -= 1000\n",
    "                    incentive += 100*(rings_new - rings)\n",
    "                if (screen_x_new - screen_x_max) < 0:\n",
    "                    incentive += 5\n",
    "                    screen_x_max = screen_x_new\n",
    "                if (y_new - y_prev) - 100 > 0:\n",
    "                    incentive += 500\n",
    "                    \n",
    "            lives = lives_new\n",
    "            rings = rings_new\n",
    "            y_prev = y_new\n",
    "                \n",
    "            agent.step(state, action, reward+incentive, next_state_w[-1], done)\n",
    "            \n",
    "            state_prev.append(state)\n",
    "            action_prev.append(action)\n",
    "            foresight.append(reward+incentive)\n",
    "            \n",
    "            if (np.array(foresight) > 0).all():\n",
    "                if np.sum(foresight) > 500:\n",
    "                    for i in range(4):\n",
    "                        agent.step(state_prev[-1-i], action_prev[-1-i], (reward+incentive)*np.power(0.8, i), state_prev[-i], done)\n",
    "            \n",
    "            my_comm.send({'img_data': imagedata })\n",
    "            imagedata = showarray(env.render(mode='rgb_array'))\n",
    "            state = next_state_w[-1]\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, np.exp(eps_decay*eps)) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if i_episode == n_episodes:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.positiveqs_local.state_dict(), 'Sonic1_GreenZone_PositiveCues.pth')\n",
    "            torch.save(agent.negativeqs_local.state_dict(), 'Sonic1_GreenZone_NegativeCues.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "# agent.positiveqs_local.load_state_dict(torch.load('Sonic1_GreenZone_PositiveCues.pth'))\n",
    "# agent.negativeqs_local.load_state_dict(torch.load('Sonic1_GreenZone_NegativeCues.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of Gym Retro.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
